/*
 * This file is part of TiPi (a Toolkit for Inverse Problems and Imaging)
 * developed by the MitiV project.
 *
 * Copyright (c) 2014 the MiTiV project, http://mitiv.univ-lyon1.fr/
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

//# // Get common definitions.
//# include <common.javax>
package mitiv.cost;

import mitiv.base.Shape;
import mitiv.base.Traits;
import mitiv.linalg.Vector;
import mitiv.linalg.VectorSpace;
import mitiv.linalg.shaped.DoubleShapedVector;
import mitiv.linalg.shaped.FloatShapedVector;
import mitiv.linalg.shaped.ShapedVectorSpace;

/**
 * Hyperbolic approximation of the total variation.
 *
 * The hyperbolic approximation of the total variation is a smooth (i.e. differentiable)
 * cost function which can also be used to implement edge-preserving smoothness.
 * The current implementation is also isotropic.
 *
 * @author Lucie Thi√©baut
 */
public class HyperbolicTotalVariation implements DifferentiableCostFunction {
    /** The vector space for the variables. */
    final protected ShapedVectorSpace inputSpace;

    /** The number of dimensions. */
    protected int rank;

    /** The data type. */
    protected int type;

    /** The dimensions of the variables. */
    protected Shape shape;

    /** The threshold */
    protected double epsilon;

    /** The scaling of the finite differences along each dimension. */
    protected double[] delta;

    public HyperbolicTotalVariation(ShapedVectorSpace inputSpace, double epsilon) {
        this.inputSpace = inputSpace;
        shape = inputSpace.getShape();
        rank = (shape == null ? 0 : shape.rank());
        type = inputSpace.getType();
        setThreshold(epsilon);
        delta = new double[rank];
        defaultScale();
    }

    public HyperbolicTotalVariation(ShapedVectorSpace inputSpace, double epsilon, double[] delta) {
        this(inputSpace, epsilon);
        setScale(delta);
    }

    public void setThreshold(double epsilon) {
        if (epsilon <= 0.0) {
            throw new IllegalArgumentException("Bad threshold value.");
        }
        this.epsilon = epsilon;
    }
    public double getThreshold() {
        return epsilon;
    }

    public void defaultScale() {
        setScale(1.0);
    }

    public void setScale(double value) {
        if (value <= 0.0) {
            throw new IllegalArgumentException("Bad scale value.");
        }
        for (int k = 0; k < rank; ++k) {
            this.delta[k] = value;
        }
    }

    public void setScale(double[] delta) {
        if (delta == null || delta.length != rank) {
            throw new IllegalArgumentException("Bad scale size.");
        }
        for (int k = 0; k < rank; ++k) {
            if (delta[k] <= 0.0) {
                throw new IllegalArgumentException("Bad scale value.");
            }
        }
        for (int k = 0; k < rank; ++k) {
            this.delta[k] = delta[k];
        }
    }
    public double getScale(int k) {
        return (k >= 0 && k < rank ? delta[k] : 1.0);
    }

    @Override
    public VectorSpace getInputSpace() {
        return inputSpace;
    }

    @Override
    public double
    computeCostAndGradient(double alpha, Vector vx, Vector vgx, boolean clr)
    {
        if (vgx != null && clr) {
            vgx.fill(0.0);
        }
        if (type == Traits.FLOAT) {
            float[] x = ((FloatShapedVector)vx).getData();
            float[] gx = null;
            if (vgx != null) {
                gx = ((FloatShapedVector)vgx).getData();
            }
            if (rank == 1) {
                return computeFloat1D(alpha, x, gx);
            } else if (rank == 2) {
                return computeFloat2D(alpha, x, gx);
            } else if (rank == 3) {
                return computeFloat3D(alpha, x, gx);
            } else {
                badRank();
            }
        } else if (type == Traits.DOUBLE) {
            double[] x = ((DoubleShapedVector)vx).getData();
            double[] gx = null;
            if (vgx != null) {
                gx = ((DoubleShapedVector)vgx).getData();
            }
            if (rank == 1) {
                return computeDouble1D(alpha, x, gx);
            } else if (rank == 2) {
                return computeDouble2D(alpha, x, gx);
            } else if (rank == 3) {
                return computeDouble3D(alpha, x, gx);
            } else {
                badRank();
            }
        } else {
            badType();
        }
        return 0.0;
    }

    protected static void badRank() {
        throw new IllegalArgumentException("Unsupported number of dimensions for Total Variation.");
    }

    protected static void badType() {
        throw new IllegalArgumentException("Unsupported data type for Total Variation.");
    }

    /*
     * The quadratic norm of the spatial gradient is the sum along all the
     * dimensions of the average squared differences along the dimension.  A
     * weight is applied along each dimension to account for the
     * regularization weight and for unequal sampling along the dimensions.
     *
     * More specifically, the regularization writes:
     *
     *     alpha*(sqrt(sd1 + sd2 + ... + sdn + eps^2) - eps)
     *
     * with alpha > 0 the regularization weight, n the rank and sdk the
     * average squared scaled differences along the kth dimension.  The
     * removal of eps is just to have the cost equal to zero for a flat array.
     *
     * For instance, in 2D, the total variation is computed for each 2x2 blocs
     * with:
     *
     *     sd1 = (((x2 - x1)/delta1)^2 + ((x4 - x3)/delta1)^2)/2
     *         = w1*((x2 - x1)^2 + (x4 - x3)^2)
     *     sd2 = (((x3 - x1)/delta2)^2 + ((x4 - x2)/delta2)^2)/2
     *         = w2*((x3 - x1)^2 + (x4 - x2)^2)
     *
     * where division by delta1 and delta2 is to scale the finite differences
     * along each dimension and dimension by 2 is to average (there 2 possible
     * finite differences in a 2x2 bloc along a given dimension).  The weights
     * w1 and w2 are given by:
     *
     *     w1 = 1/(2*delta1^2)
     *     w2 = 1/(2*delta2^2)
     *     s = eps^2
     */

    //# for id in ${FLOAT} ${DOUBLE}
    //#     def type = ${}{type_${id}}
    //#     def Type = ${}{Type_${id}}
    //#     def TYPE = ${}{TYPE_${id}}
    private final double compute${Type}1D(double alpha, ${type}[] x, ${type}[] gx)
    {
        /* Note that ALPHA is not taken into account when summing FCOST (this
         * is done at the end) while ALPHA is taken into account when
         * integrating the gradient GCOST.  The sum is done in double
         * precision whatever the type. */
        final boolean computeGradient = (gx != null);
        final int dim1 = shape.dimension(0);
        final ${type} s = (${type})square(epsilon*delta[0]);
        double fcost = 0.0;
        ${type} beta = (${type})(alpha/delta[0]);
        for (int i1 = 1; i1 < dim1; ++i1) {
            ${type} d = x[i1] - x[i1 - 1];
            ${type} r = sqrt(d*d + s);
            fcost += r;
            if (computeGradient) {
                ${type} p = beta*(d/r);
                gx[i1 - 1] -= p;
                gx[i1] += p;
            }
        }

        /* Remove the "bias" and make sure the result is non-negative (it
           can only be negative due to rounding errors). */
        fcost = (fcost/delta[0]) - (dim1 - 1)*epsilon;
        if (fcost < 0.0) {
            fcost = 0.0;
        }
        return alpha*fcost;
    }
    //# end

    //# for id in ${FLOAT} ${DOUBLE}
    //#     def type = ${}{type_${id}}
    //#     def Type = ${}{Type_${id}}
    //#     def TYPE = ${}{TYPE_${id}}
    private final double compute${Type}2D(double alpha, ${type}[] x, ${type}[] gx)
    {
        /* Note that ALPHA is not taken into account when summing FCOST (this
         * is done at the end) while ALPHA is taken into account when
         * integrating the gradient GCOST.  The sum is done in double
         * precision whatever the type. */
        final boolean computeGradient = (gx != null);
        final int dim1 = shape.dimension(0);
        final int dim2 = shape.dimension(1);
        final ${type} w1 = (${type})(1.0/(2.0*square(delta[0])));
        final ${type} w2 = (${type})(1.0/(2.0*square(delta[1])));
        final ${type} s = (${type})square(epsilon);
        double fcost = 0.0;
        //# if ${id} != ${DOUBLE}
        //#     def alpha = _alpha
        ${type} ${alpha} = (${type})alpha;
        //# else
        //#     def alpha = alpha
        //# end
        ${type} x1, x2, x3, x4;
        int j1, j2, j3, j4;
        if (w1 == w2) /* same weights along all directions */ {
            final ${type} w = w1;
            for (int i2 = 1; i2 < dim2; ++i2) {
                j2 = (i2 - 1)*dim1;
                j4 = i2*dim1;
                x2 = x[j2];
                x4 = x[j4];
                for (int i1 = 1; i1 < dim1; ++i1) {
                    j1 = j2++;
                    j3 = j4++;
                    x1 = x2;
                    x2 = x[j2];
                    x3 = x4;
                    x4 = x[j4];
                    ${type} y21 = x2 - x1;
                    ${type} y43 = x4 - x3;
                    ${type} y31 = x3 - x1;
                    ${type} y42 = x4 - x2;
                    ${type} r = sqrt((square(y21) + square(y43)
                            + square(y31) + square(y42))*w + s);
                    fcost += r;
                    if (computeGradient) {
                        ${type} p = ${alpha}*w/r;
                        gx[j1] = gx[j1] - (y21 + y31)*p;
                        gx[j2] = gx[j2] + (y21 - y42)*p;
                        gx[j3] = gx[j3] - (y43 - y31)*p;
                        gx[j4] = gx[j4] + (y43 + y42)*p;
                    }
                }
            }
        } else /* not same weights along all directions */ {
            for (int i2 = 1; i2 < dim2; ++i2) {
                // Put 2x2 bloc at start of a new line.  In the code, (i1,i2) is
                // 2D coordinates, j1, j2, j3 and j4 are the 1D indices of the 2x2
                // elements.
                j2 = (i2 - 1)*dim1;
                j4 = i2*dim1;
                x2 = x[j2];
                x4 = x[j4];
                for (int i1 = 1; i1 < dim1; ++i1) {
                    // Move to next 2x2 bloc.
                    // +---+---+
                    // | 3 | 4 |
                    // +---+---+
                    // | 1 | 2 |
                    // +---+---+
                    j1 = j2++;
                    x1 = x2;
                    x2 = x[j2];
                    j3 = j4++;
                    x3 = x4;
                    x4 = x[j4];

                    // Compute horizontal and vertical differences.
                    ${type} y21 = x2 - x1;
                    ${type} y43 = x4 - x3;
                    ${type} y31 = x3 - x1;
                    ${type} y42 = x4 - x2;

                    // Compute hyperbolic approximation of L2 norm of
                    // the spatial gradient.
                    ${type} r = sqrt((square(y21) + square(y43))*w1 +
                            (square(y31) + square(y42))*w2 + s);
                    fcost += r;
                    if (computeGradient) {
                        ${type} q = ${alpha}/r;
                        ${type} p1 = w1*q;
                        y21 *= p1;
                        y43 *= p1;
                        ${type} p2 = w2*q;
                        y31 *= p2;
                        y42 *= p2;
                        gx[j1] = gx[j1] - (y21 + y31);
                        gx[j2] = gx[j2] + (y21 - y42);
                        gx[j3] = gx[j3] - (y43 - y31);
                        gx[j4] = gx[j4] + (y43 + y42);
                    }
                }
            }
        }

        /* Remove the "bias" and make sure the result is non-negative (it
           can only be negative due to rounding errors). */
        fcost -= (dim1 - 1)*(dim2 - 1)*epsilon;
        if (fcost < 0.0) {
            fcost = 0.0;
        }
        return alpha*fcost;
    }
    //# end

    /*
     * NOTATIONS FOR 3-D VOLUME X(i1,i2,i3)
     *
     *                 i3  i2
     *                  | /
     *                  |/              X1 = X(i1-1,i2-1,i3-1)
     *        X7--------X8---> i1       X2 = X(i1  ,i2-1,i3-1)
     *       /:        /|               X3 = X(i1-1,i2  ,i3-1)
     *      / :       / |               X4 = X(i1  ,i2  ,i3-1)
     *     X5--------X6 |               X5 = X(i1-1,i2-1,i3  )
     *     |  X3.....|..X4              X6 = X(i1  ,i2-1,i3  )
     *     | '       | /                X7 = X(i1-1,i2  ,i3  )
     *     |'        |/                 X8 = X(i1  ,i2  ,i3  )
     *     X1--------X2
     *
     */
    //
    //# for id in ${FLOAT} ${DOUBLE}
    //#     def type = ${}{type_${id}}
    //#     def Type = ${}{Type_${id}}
    //#     def TYPE = ${}{TYPE_${id}}
    private final double compute${Type}3D(double alpha, ${type}[] x, ${type}[] gx)
    {
        /* Note that ALPHA is not taken into account when summing FCOST (this
         * is done at the end) while ALPHA is taken into account when
         * integrating the gradient GCOST.  The sum is done in double
         * precision whatever the type. */
        final boolean computeGradient = (gx != null);
        final int dim1 = shape.dimension(0);
        final int dim2 = shape.dimension(1);
        final int dim3 = shape.dimension(2);
        final ${type} w1 = (${type})(1.0/(2.0*square(delta[0])));
        final ${type} w2 = (${type})(1.0/(2.0*square(delta[1])));
        final ${type} w3 = (${type})(1.0/(2.0*square(delta[2])));
        final ${type} s = (${type})square(epsilon);

        // The sum is done in double precision whatever the type.
        double fcost = 0.0;
        //# if ${id} != ${DOUBLE}
        //#     def alpha = _alpha
        ${type} ${alpha} = (${type})alpha;
        //# else
        //#     def alpha = alpha
        //# end
        ${type} x1, x2, x3, x4, x5, x6, x7, x8;
        int j1, j2, j3, j4, j5, j6, j7, j8;
        for (int i3 = 1; i3 < dim3; ++i3) {
            for (int i2 = 1; i2 < dim2; ++i2) {
                // Put 2x2x2 bloc such that 8th point is at coordinates (0,i2,i3).
                j2 = (i2 - 1 + (i3 - 1)*dim2)*dim1; // (0,i2-1,i3-1)
                j4 = (i2     + (i3 - 1)*dim2)*dim1; // (0,i2,i3-1)
                j6 = (i2 - 1 + (i3    )*dim2)*dim1; // (0,i2-1,i3)
                j8 = (i2     + (i3    )*dim2)*dim1; // (0,i2,i3)
                x2 = x[j2];
                x4 = x[j4];
                x6 = x[j6];
                x8 = x[j8];
                for (int i1 = 1; i1 < dim1; ++i1) {
                    // Move to next 2x2x2 bloc.
                    j1 = j2++;
                    x1 = x2;
                    x2 = x[j2];
                    j3 = j4++;
                    x3 = x4;
                    x4 = x[j4];
                    j5 = j6++;
                    x5 = x6;
                    x6 = x[j6];
                    j7 = j8++;
                    x7 = x8;
                    x8 = x[j8];

                    // Compute differences along 1st dimension.
                    ${type} y21 = x2 - x1;
                    ${type} y43 = x4 - x3;
                    ${type} y65 = x6 - x5;
                    ${type} y87 = x8 - x7;
                    ${type} r1 = square(y21) + square(y43) + square(y65) + square(y87);

                    // Compute differences along 2nd dimension.
                    ${type} y31 = x3 - x1;
                    ${type} y42 = x4 - x2;
                    ${type} y75 = x7 - x5;
                    ${type} y86 = x8 - x6;
                    ${type} r2 = square(y31) + square(y42) + square(y75) + square(y86);

                    // Compute differences along 3rd dimension.
                    ${type} y51 = x5 - x1;
                    ${type} y62 = x6 - x2;
                    ${type} y73 = x7 - x3;
                    ${type} y84 = x8 - x4;
                    ${type} r3 = square(y51) + square(y62) + square(y73) + square(y84);

                    // Compute hyperbolic approximation of L2 norm of
                    // the spatial gradient.
                    ${type} r = sqrt(w1*r1 + w2*r2 + w3*r3 + s);
                    fcost += r;
                    if (computeGradient) {
                        ${type} q = ${alpha}/r;
                        ${type} p1 = w1*q;
                        y21 *= p1;
                        y43 *= p1;
                        y65 *= p1;
                        y87 *= p1;
                        ${type} p2 = w2*q;
                        y31 *= p2;
                        y42 *= p2;
                        y75 *= p2;
                        y86 *= p2;
                        ${type} p3 = w3*q;
                        y51 *= p3;
                        y62 *= p3;
                        y73 *= p3;
                        y84 *= p3;
                        gx[j1] = gx[j1] - (y21 + y31 + y51);
                        gx[j2] = gx[j2] + (y21 - y42 - y62);
                        gx[j3] = gx[j3] - (y43 - y31 + y73);
                        gx[j4] = gx[j4] + (y43 + y42 - y84);
                        gx[j5] = gx[j5] - (y65 + y75 - y51);
                        gx[j6] = gx[j6] + (y65 - y86 + y62);
                        gx[j7] = gx[j7] - (y87 - y75 - y73);
                        gx[j8] = gx[j8] + (y87 + y86 + y84);
                    }
                }
            }
        }

        /* Remove the "bias" and make sure the result is non-negative (it
           can only be negative due to rounding errors). */
        fcost -= (dim1 - 1)*(dim2 - 1)*(dim3 - 1)*epsilon;
        if (fcost < 0.0) {
            fcost = 0.0;
        }
        return alpha*fcost;
    }
    //# end

    static final private float sqrt(float a) {
        return (float)Math.sqrt(a);
    }

    static final private double sqrt(double a) {
        return Math.sqrt(a);
    }

    static final private float square(float a) {
        return a*a;
    }

    static final private double square(double a) {
        return a*a;
    }

    @Override
    public double evaluate(double alpha, Vector x) {
        return computeCostAndGradient(alpha, x, null, false);
    }
}

/*
 * Local Variables:
 * mode: Java
 * tab-width: 8
 * indent-tabs-mode: nil
 * c-basic-offset: 4
 * fill-column: 78
 * coding: utf-8
 * ispell-local-dictionary: "american"
 * End:
 */
